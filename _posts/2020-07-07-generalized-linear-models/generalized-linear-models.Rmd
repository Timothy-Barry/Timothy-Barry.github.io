---
title: "Generalized linear models"
description: |
  A dive into the theory behind GLMs. This post covers the GLM model, canonical and non-canonical link functions, optimization of the log-likelihood, and inference.
author:
  - name: Tim Barry
    url: https://timothy-barry.github.io
date: 07-07-2020
output:
  distill::distill_article:
    self_contained: false
preview: log_reg.png
categories:
  - Statistics
draft: true
---

I like the CMU Statistics Department very much. However, the department does not really teach students about GLMs. Given the popularity of GLMs in application, this makes me scratch my head a bit. This post is my effort to understand GLMs. I explore various topics, including canonical and non-canonical link functions, optimization of the log likelihood, and inference. Please let me know (via an email or a tap on the shoulder when I am at my desk\*) if I've gotten anything wrong!

\* You alternately can throw a crumpled piece of paper at me if you'd like to maintain social distancing.

## Exponential family review

Let $\{P_\theta\}, \theta \in \Theta \subset \mathbb{R}$ be a family of distributions. Recall that $\{P_\theta\}$ belongs to the 1-parameter exponential family if its density can be written as
$$f(y | \theta) = e^{ \eta(\theta) T(y) - \psi(\theta)} h(y).$$ 

We can reparameterize the exponential family so that its density is in canonical form:
$$f(y|\eta) = e^{ \eta T(y) - \psi(\eta)}h(y).$$
In this post we will assume the data $\{y_1, \dots, y_n\}$ come from a slightly restricted version of the exponential family. Specifically, we will assume the $y_i$s have density $$f(y_i | \eta_i) = e^{\eta_i y_i - \psi(\eta_i)}h(y_i),$$ i.e., we will assume the sufficient statistic $T(y_i)$ is the identity. This slightly narrower class encompasses the most important distributions for statistical modeling, including the binomial, exponential, Poisson, Bernoulli, negative binomial (assuming fixed number of failures), and normal (assuming unit variance) distributions. Note that the function $h$ is not a function of the unknown parameter $\eta_i$ and thus will show up as a constant in the log-likelihood. The only function that contains information (relevant to inference) about the distribution is the cumulant generating function $\psi$. Thus, we can identify exponential families (with identity sufficient statistic) with their corresponding canonical-form cumulant generating function. 

## GLM model

We observe independent (but not necessarily identically-distributed) tuples $\{(y_1, x_1), \dots, (y_n, x_n)\},$ where $y_i \in \mathbb{R}$ and $x_i \in \mathbb{R}^p$. Assume the $y_i$s are drawn from a one-parameter exponential family with identity sufficient statistic and possibly different canonical parameters: $$ y_i \sim f(y_i|\eta_i) = e^{ \eta_i y_i - \psi(\eta_i)}h(y).$$ Assume the $x_i$s are fixed and known. Recall that in exponential families, there exists a bijective map between the natural parameter and the mean. Thus, the observations $\{y_1, \dots, y_n\}$ have means $\{ \mu_1, \dots, \mu_n \}.$

We must link the responses $y_i$ to the covariates $x_i$. Assume there exists a strictly increasing and differentiable function $g: \mathbb{R} \to \mathbb{R}$ and an unknown vector $\beta \in \mathbb{R}^p$ such that 
$$ g(\mu_i) = \langle x_i, \beta \rangle $$ for all $i \in \{ 1, \dots, n \}$. The function $g$ is called the **link function**, and the function $g^{-1}$ is called the **inverse link function**. With these various pieces in place, we can frame our statistical problem as follows: given data $\{(y_i, x_i)\}_{i=1}^n$, a link function $g$, and an exponential family density $f$, estimate the unknown parameter $\beta$. We will estimate $\beta$ and obtain associated standard errors through MLE.

## Canonical link function

For a given exponential family, there exists a special link function called the **canonical link function** that imbues the GLM with very nice mathematical properties. Use of the canonical link is entirely optional; however, when given the option, people generally choose to use the canonical link given its nice properties.

The canonical link function is the link function that results from setting the linear component of the model ($\langle x_i, \beta_i \rangle$) equal to the natural parameter ($\eta_i$). In math, the canonical link function is that function $g_c$ that satisfies
$$g_c(\mu_i) = \langle x_i, \beta \rangle = \eta_i.$$
We can express the canonical link function in terms of known quantities. Recall that for an exponential family in canonical form (with identity sufficient statistic), we have $\psi'(\eta_i) = \mathbb{E}[Y_i] = \mu_i.$ Therefore, by the definition of $g_c$ above, we obtain $g_c = [\psi']^{-1}.$ That is, the canonical link function is equal to the inverse of the derivative of $\psi$.

The main advantage of using the canonical link function is that the canonical link renders the canonical parameter of the joint distribution $y = (y_1, \dots, y_n)^T$ equal to $\beta$.

**Theorem**: *Suppose we model the mean $\mu_i$ of $y_i$ using the canonical link function, i.e. $g_c(\mu_i) = \langle x_i,\beta \rangle$. Then the joint density of $y = (y_1, \dots, y_n)^T$ is a p-parameter exponential family with canonical parameter $\beta$.*

**Proof sketch**: Define $\eta = X \beta$, where $\eta = [\eta_1, \dots, \eta_n]^T \in \mathbb{R}^n$ and $X$ is the $n \times p$ design matrix of observations. Let $[S_1(y), \dots, S_p(y)]^T$ denote the $p$-dimensional product of the matrix-vector multiplication $X^T y$. Observe that
$$ \sum_{i=1}^n \eta_i y_i = \sum_{i=1}^n x_i^T \beta y_i = \beta^T X^T y = \langle \beta, [S_1(y), \dots, S_p(y)]^T \rangle.$$
Next, define $\phi: \mathbb{R}^p \to \mathbb{R}$ by $$\phi(\beta) = \sum_{i=1}^n \psi(x_i^T \beta) = \sum_{i=1}^n \psi(\eta_i).$$ Finally, define
$$H(y) = \prod_{i=1}^n h(y_i).$$
We can express the joint density $f_c$ of $y = (y_1, \dots, y_n)^T$ as
$$f_c(y|\beta) = e^{ \sum_{i=1}^n \eta_i y_i - \psi(\eta_i)}\prod_{i=1}^n h(y_i) = \left( e^{\langle \beta, [S_1(y), \dots, S_p(y)]^T \rangle - \phi(\beta)}\right) H(y).$$ $\square$

We see that $f_c$ is a $p$-dimensional exponential family density in canonical form. We have that

* $\beta$ is the $p$-dimensional canonical parameter
* $[S_1(y), \dots, S_p(y)]^T$ is the p-dimensional sufficient statistic
* $\phi$ is the cumulant generating function
* $H$ is the carrier density.

The density $f_c(y|\beta)$ inherits all the nice properties of a $p$-dimensional exponential family density in canonical form, including convexity of the log-likelihood and equality of the observed and Fisher information matrices:

1. **Convexity**. The log-likelihood for $\beta$ is a concave function defined over a convex set. Typically, the log-likelihood for $\beta$ is strictly concave (although this depends on $\phi$). Thus, the MLE for $\beta$ exists, (typically) is unique, and is easy to compute.

2. **Equality of observed and Fisher information matrices**. The observed and Fisher information matrices for $\beta$ coincide. This fact simplifies some aspects of GLM fitting and inference, as we will see.

## Non-canonical link function

We can use a link function that is non-canonical. Consider the same setup as before (i.e., the problem setup as presented in the *GLM model* section.) Given an exponential family distribution, a link function, and set of data, our goal is to estimate the unknown parameter $\beta$ of the GLM through MLE.


We begin by introducing some notation. Let $h : \mathbb{R} \to \mathbb{R}$ be the function that maps the linear predictor into the canonical parameter, i.e. $$h( \langle x_i, \beta \rangle ) = \eta_i.$$ The function $h$ is simply the identity function when we use the canonical link. In general, we can express $h$ in terms of $g$ and $\psi$:
$$ h( \langle x_i, \beta \rangle) = \eta_i = [\psi']^{-1}(\mu_i) = [\psi']^{-1}(g^{-1}(\langle x_i, \beta \rangle)).$$ Thus, $h = [\psi']^{-1} \circ g^{-1}.$

At this point we have defined a lot of functions. There are three functions in particular that are relevant to our analysis: the cumulant generating function $\psi$, the link function $g$, and the above-defined function $h$. We review the definitions and properties of these functions in the list below.

1. Function $\psi$
  + Definition: the cumulant generating function of the exponential family in canonical form.
  + Properties: $\psi'(\eta_i) = \mu_i$ and $\psi''(\eta_i) = \sigma_i^2$, where $\sigma_i^2 = \mathbb{V}(y_i)$.

2. Function $g$
  + Definition: the function that maps the mean to the linear component, i.e. $g(\mu_i) = \langle x_i, \beta \rangle$.
  + Properties: The canonical link function $g_c$ satisfies $g_c = [\psi']^{-1}$.

3. Function $h$
  + Definition: the function that maps the linear component to the canonical parameter, i.e. $h(\langle x_i, \beta \rangle) = \eta_i$.
  + Properties: $h$ can be expressed in terms of $\psi$ and $g$ as $h = [\psi']^{-1} \circ g^{-1}$. When $g$ is the canonical link, $h$ is the identity.

With these definitions in hand, we can express the log-likelihood of the model (up to a constant) as follows:
$$\mathcal{L}(\beta;y,X) = \sum_{i=1}^n \eta_i y_i - \psi(\eta_i) = \sum_{i=1}^n y_i \cdot h(\langle x_i, \beta \rangle) - \psi( h(\langle x_i, \beta \rangle)).$$
For optimization and inference purposes, we need to compute the gradient and Hessian of the log-likelihood. Recall that the gradient of the log-likelihood is the score statistic, the Hessian of the log-likelihood is the negative observed information matrix, and the expected Hessian of the log-likelihood is the negative Fisher information matrix. We need to define some matrices and vectors to express these quantities compactly. Yes, this is painful, but it is necessary. Define the matrices

$$
\begin{cases}
\Delta = \textrm{diag}\left\{ h'( \langle x_i, \beta \rangle ) \right\}_{i=1}^n \\
\Delta' = \textrm{diag}\left\{ h''( \langle x_i, \beta \rangle) \right\}_{i=1}^n \\
V = \textrm{diag}\left\{ \psi''(\eta_i) \right\}_{i=1}^n \\
H = \textrm{diag}\left\{ y_i - \mu_i \right\}_{i=1}^n \\
W = \Delta V \Delta.
\end{cases}
$$ Also, define the vector $s = [ y_1 - \mu_1, \dots, y_n - \mu_n]^T.$ We can show through calculus (use the chain rule!) that
$$ \nabla \mathcal{L}(\beta) = X^T \Delta s$$ and
$$ \nabla^2 \mathcal{L}(\beta) = - X^T (\Delta V \Delta - \Delta'H ) X.$$

Suppose we use the canonical link function. Then $h$ is the identity, implying $h'$ is identically equal to $1$ and $h''$ is identically equal to $0$. Thus, $\Delta = I$ and $\Delta' = 0$, and we get a simpler expression for the observed information matrix:
$$\nabla^2 \mathcal{L}(\beta) = -X^TVX.$$

We can compute the Fisher information matrix $I(\beta)$ by taking the expectation of the observed information matrix:
$$ I(\beta) = \mathbb{E} \left[ \nabla^2 \mathcal{L}(\beta) \right] = -X^T ( \Delta V \Delta - \Delta' \mathbb{E}[H])X = -X^T ( \Delta V \Delta)X = -X^TWX.$$

Note that the observed and Fisher matrices coincide if we use the canonical link function (as predicted by exponential family theory).

## Optimization

We can optimize the log likelihood through one of three related methods: Newton-Raphson, Fisher scoring, or iteratively reweighted least squares. We discuss these methods seriatim.

**Newton-Raphson**: Newton-Raphson is a general optimization algorithm. Let $f: \mathbb{R}^p \to \mathbb{R}$ be a twice continuously differentiable, concave function. The following iterative algorithm converges to the global maximum of $f$:
$$ x^{(k+1)} \leftarrow x^{(k)} - [\nabla^2 f(x^{(k)})]^{-1} [ \nabla f(x^{(k)})].$$
To use this algorithm to optimize $\mathcal{L},$ substitute the negative observed information matrix for $\nabla^2 f(x^{(k)})$ and the score statistic for $\nabla f(x^{(k)})$:
$$ \beta^{(k+1)} \leftarrow \beta^{(k)} + \left[ X^T (\Delta V \Delta - \Delta'H ) X \right]^{-1} X^T \Delta s |_{\beta = \beta^{(k)}}.$$

**Fisher scoring**: The Fisher information matrix is the expected observed information matrix. It turns out that we can replace the observed information matrix with the Fisher information matrix in the Newton-Raphson algorithm and retain global convergence. Making this substitution, we obtain
$$ \beta^{(k+1)} \leftarrow \beta^{(k)} + [ X^T W X]^{-1} [ X^T \Delta s]|_{ \beta = \beta^{(k)} }.$$
This modified algorithm is known as the Fisher scoring algorithm.

**Iteratively reweighted least squares**. We can rewrite the Fisher scoring algorithm in a clever way to derive a fast implementation of the algorithm. Define $$\tilde{y} = [g'(\mu_1)y_1, \dots, g'(\mu_n)y_n]^T$$ and $$\tilde{\mu} = [g'(\mu_1) \mu_1, \dots, g'(\mu_n)\mu_n]^T.$$  We begin with a lemma.

**Lemma**: $\Delta s = W(\tilde{y} - \tilde{\mu})$.

**Proof**: Recall that $g^{-1}(t) = \psi'(h(t)).$ By the inverse derivative theorem,
$$ \frac{1}{g'(g^{-1}(t))} = \psi''(h(t))h'(t).$$ Plugging in $\langle X_i, \beta \rangle$ for $t$, we find
$$ \begin{multline} \frac{1}{g'(g^{-1}(\langle X_i, \beta \rangle))} = \psi''(h(\langle X_i, \beta \rangle)) h'(\langle X_i, \beta \rangle) \iff \psi''(\eta_i) = \frac{1}{ g'(\mu_i) h'( \langle X_i, \beta \rangle)} .\end{multline}$$
Next, note that $W = \Delta V \Delta = \textrm{diag}\left\{ h'( \langle X_i, \beta \rangle )^2 \psi''(\eta_i)  \right\}_{i=1}^n.$ Plugging in the expression we derived for $\psi''(\eta_i)$ above, we obtain
$$ W = \textrm{diag}\left\{ \frac{h'(\langle X_i, \beta \rangle)}{ g'(\mu_i) } \right\}_{i=1}^n.$$ Finally, it is clear that
$$\begin{multline}
\Delta s = \textrm{diag}\{ h'(\langle X_i, \beta \rangle)\}_{i=1}^n ([ y_1 - \mu_1, \dots, y_n - \mu_n]) \\ = \textrm{diag}\left\{ \frac{h'(\langle X_i, \beta \rangle)}{ g'(\mu_i) } \right\}_{i=1}^n ( g'(\mu_1)[y_1 - \mu_1], \dots, g'(\mu_n)[y_n - \mu_n]) = W(\tilde{y} - \tilde{\mu}).
\end{multline}$$ $\square$ 

Returning to the optimization problem, we can rewrite Fisher's scoring algorithm as
$$ \beta^{(k+1)} \leftarrow \beta^{(k)} + (X^TWX)^{-1} X^T \Delta s\\ = (X^TWX)^{-1} X^TW(\tilde{y} - \tilde{\mu}) \\ = (X^TWX)^{-1} X^TW(\tilde{y} - \tilde{\mu} + X\beta^{(k)}).$$ 

Recall the weighted least squares problem. Given an objective function $f: \mathbb{R}^p \to \mathbb{R}$ defined by
$$ f(\beta) = (y - X \beta) W (y - X\beta)$$ for an $n \times p$ design matrix $X$, a diagonal matrix of weights $W$, and a response vector $y$, the global minimizer is
$$ \hat{\beta} = (X^TWX)^{-1}X^TWy.$$
We can use a weighted least squares solver to compute $\beta^{(k+1)}$: set the design matrix equal to $X$, the diagonal matrix of weights equal to $W$, and the response vector equal to $\tilde{y} - \tilde{\mu} + X\beta^{(k)}$. This procedure is called iteratively reweighted least squares (IRLS).

R uses IRLS to implement the *glm* function. R initializes the parameter $\beta^{(0)}$ to a random value. R then repeatedly solves weighted least squares problems until the sequence $\{ \beta^{(0)}, \beta^{(1)}, \dots \}$ converges.

## Inference

We can construct standard errors for the estimated parameter $\hat{\beta}$ using MLE asymptotics. Recall that $I(\beta)$ is the Fisher information matrix of the model evaluated at $\beta$. If the number of samples $n$ is large, we have the approximation
$$ \hat{\beta} \sim N_p(\beta, I(\hat{\beta})).$$ We can compute $I(\hat{\beta})$ according to the formula $I(\hat{\beta}) = X^T W(\hat{\beta})X,$ where $W(\hat{\beta})$ is the weight matrix evaluated at $\hat{\beta}$. Note that R returns the matrix $W(\hat{\beta})$ as part of the *glm* output.

## Conclusion

This ends our three-part mini series on exponential families, information matrices, and GLMs. We saw that GLMs, though old, are an elegant, general, and powerful method for modeling and inference. In later posts we will study how GLMs can be used to model genomic and genetic data, with an eye toward single-cell and bulk-tissue RNA-seq. But first we will take a high-dimensional detour.

<center>
![](meme.png){#id .class width=40%}

*9gag*
</center>

## References

1. [Lecture Notes](http://statweb.stanford.edu/~ckirby/brad/STATS305B_Part-3_corrected-2.pdf) provided by Professor Bradley Efron.
2. [Lecture Notes](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_GLM.pdf) provided by Professor Philippe Rigollet.
3. Ibrahim, Joseph G. ["Incomplete data in generalized linear models."](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1990.10474938?casa_token=vfq9cW4fofUAAAAA:hA7ZDlyPg_IYhhMRyWlKu3pVaKXHma1PPOfLySP40mFUPb4sQoZINn7fPTfuGKxTXZhcoAhzxch3#.XwqISC2z1TY) Journal of the American Statistical Association 85.411 (1990): 765-769.